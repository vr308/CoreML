#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Sun Oct 15 16:41:51 2017
@author:  vr308
"""
import numpy as np
import matplotlib.pylab as plt
import scipy.stats as st
import GPy as gp
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as Ck
from matplotlib import cm
import matplotlib as mpl


M = 3
cov = np.identity(3)
x = np.sort(np.random.uniform(-8,8, 200))

# Sampling from a univariate gaussian

x=np.arange(-5,+5,0.01)
pdf=st.norm.pdf(x, loc=0,scale=1)
s = np.random.normal(0, 1, 25)

plt.figure()
plt.plot(x,pdf)
plt.title('Samples from a Gaussian distributed random variable $x \sim N(0,1)$', fontsize='x-small')
plt.xlabel('x')
plt.scatter(s,np.zeros(25), marker='x')

# Sampling from a GP prior 

k = gp.kern.RBF(input_dim=1,lengthscale=0.2)
k = gp.kern.Brownian()
k = gp.kern.Matern32(input_dim=1, variance=1, lengthscale=0.2)


# Select input points
X = np.linspace(0.,1.,500) # define X to be 500 points evenly spaced over [0,1]
X = X[:,None] 

# Set the mean and covariance function 

mu = np.zeros((500))
C = k.K(X,X)

# Generate 20 samples from the multivariate Gaussian with mean mu and covariance C

Z = np.random.multivariate_normal(mu,C,20)

plt.figure(figsize=(5,5))    
for i in range(20):
    plt.plot(X[:],Z[i,:])
plt.xlabel('x')
plt.ylabel('f(x)')
#plt.title('Samples from ' + r'$\mu(x) = 0$' + ' and '+ r'$k(x_{i}, x_{j}) = \frac{-1}{2l^{2}}(x_{i} - x_{j})^{2}$'  + ' and 'r'$l = 0.2$' + '[RBF kernel]', fontsize='x-small')
#plt.title('Samples from ' + r'$\mu(x) = 0$' + ' and '+ r'$k(x_{i}, x_{j}) = min(x_{i}, x_{j})$' + '[Brownian kernel]', fontsize='x-small')
plt.title('Samples from ' + r'$\mu(x) = 0$' + ' and '+ r'$k(x_{i}, x_{j}) = (1 + \sqrt{3}r)\exp(-\sqrt{3}r)$' + ' and 'r'$r = \sqrt{\sum_{i=1}^{n}\frac{x_{i} - y_{i}}{l_{i}^{2}}}$' + '[Matern32 kernel]', fontsize='x-small')

# Basis function representation of a GP

def sample_paths(x, cov, gamma):
    w=np.matrix(np.random.multivariate_normal([0,0,0],cov)).T
    Phi = np.matrix([phi_gauss(x, -4, gamma), phi_gauss(x, 0, gamma), phi_gauss(x, 4, gamma)]).T
    f_x = np.matmul(Phi,w)
    return f_x
    
def phi_gauss(x,m, gamma):
    return np.exp(-gamma*np.square(x - m))
    
   
mu = [-4,0,4.0]
gamma = [0.2,0.2,0.2]
x = np.sort(np.random.uniform(-8,8, 200))
plt.figure(figsize=(8,8))
plt.subplot(211)
for i in np.arange(0,len(mu)):
    plt.plot(x, phi_gauss(x,mu[i], gamma[i]), label= '($\mu_{%d}$,$\gamma_{%d}$) = [%d, %.2f]'%(i+1,i+1, mu[i], gamma[i]))
plt.title('Basis space $\Phi = [\phi_{1}(x), \phi_{2}(x), \phi_{3}(x)]$', fontsize='small')
plt.legend(fontsize='x-small')
plt.xlabel('$x$')
plt.ylabel('$\phi_{i}(x)$')
plt.subplot(212)
for i in np.arange(1,7):
    plt.plot(x, sample_paths(x, cov, gamma[0]))
plt.title('Sample curves $f(x) = \sum_{i=1}^{3}w_{i}\phi_{i}(x)$ generated by sampling $\mathbf{w} \sim N(0,1I)$ from its prior dist.', fontsize='small')
plt.xlabel('$x$')
plt.ylabel('$f(x)$')


# GP Regression (using GPY package - does not give error bars when noise is 0 aaaargh!)

X = np.linspace(0.05,0.95,10)[:,None]
Y = -np.cos(np.pi*X) + np.sin(4*np.pi*X)
Y_noise = -np.cos(np.pi*X) + np.sin(4*np.pi*X) + np.random.normal(loc=0.0, scale=0.1, size=(10,1)) 

k = gp.kern.RBF(input_dim=1, variance=1, lengthscale=1)
m = gp.models.GPRegression(X,Y,k)

m.Gaussian_noise = 0 #variance
m.rbf.lengthscale = 1
m.plot()
plt.title('lengthscale=0.2')
print(m)

# Sklearn version of GPs

# Noise-less case

def f(x):
    """The function to predict."""
    return x * np.sin(x)

def plot_gp_fit_predict(X, x, y, y_pred, sigma, knots, y_pred_knots, sigma_knots, title, gpr):
    
    y_upper = y_pred + 2*sigma
    y_lower = y_pred - 2*sigma
    z = y_upper - y_lower  
    
    cmap = cm.get_cmap('viridis')
    x_array = x.reshape(len(y_pred),)
    
    plt.figure()
    plt.plot(x, f(x), 'r:', label=u'$f(x) = x\,\sin(x)$')
    plt.plot(X, y, 'r.', markersize=10, label=u'Observations')
    plt.plot(x, y_pred, 'b-', label=u'Prediction')
    normalize = mpl.colors.Normalize(vmin = z.min(), vmax=z.max())
    for i in range(999):
        plt.fill_between([x_array[i],x_array[i+1]], y_lower[i], y_upper[i], color = cmap(normalize(z[i])), alpha=0.7)
    plt.plot(x, gpr.sample_y(x, 10), alpha=0.5, linewidth=0.5)
    plt.errorbar(knots, y_pred_knots, yerr= sigma_knots*2, barsabove=True, ls='None', marker='s', capsize=10, color='orange')
    plt.xlabel('$x$')
    plt.ylabel('$f(x)$')
    plt.ylim(-10, 20)
    plt.legend(loc='upper left')
    plt.title(title, fontsize='small')
    
def get_max_entropy(sigma, x, y_pred):
    
    pos_max_sigma = np.where(sigma == np.max(sigma))[0]
    if len(pos_max_sigma) > 1:
        x_ = x[pos_max_sigma].reshape(len(pos_max_sigma),)
        if np.where(np.diff(pos_max_sigma) > 1)[0].size > 0:
            print('There are more than 1 regions of max entropy, identifying the centers of each region')
            break_points = np.where(np.diff(pos_max_sigma) > 1)[0]
            regions = np.split(x_,break_points+1)
            return [np.median(k) for k in regions]
        else:
            print('There is 1 region of max entropy, returning its center')
            return np.median(x[pos_max_sigma])
    else:
        print('There is one distinct point of highest variance')
        return x[pos_max_sigma]
        
#  First the noiseless case # ----------------------------------------------------------------------

X = np.atleast_2d([1., 3., 5., 6., 7., 8.]).T


i = 0
seq_ = [3.4]
while i < 5:

    X = np.atleast_2d(seq_).T 
    
    # Observations
    y = f(X).ravel()
    
    # Mesh the input space for evaluations of the real function
    x = np.atleast_2d(np.linspace(0, 10, 1000)).T
    
    # Instansiate a Gaussian Process model
    kernel = Ck(1.0, (1e-3, 1e3)) * RBF(0.97, (1e-2, 1e2))
    gpr = GaussianProcessRegressor(kernel=kernel, alpha=1e-2, optimizer=None)
    
    # Fit to data using Maximum Likelihood Estimation of the parameters
    gpr.fit(X, y)
    
    # Make the prediction on the meshed x-axis 
    y_pred, sigma = gpr.predict(x, return_std = True)
    sigma = np.round(sigma, 2)
    
    # Predict on the knots
    knots = get_max_entropy(sigma, x, y_pred)
    y_pred_knots, sigma_knots = gpr.predict(np.atleast_2d(knots).T, return_std = True)
    
    title = 'GP Regression ' + 'Iteration ' + str(i)
    plot_gp_fit_predict(X, x, y, y_pred, sigma, knots, y_pred_knots, sigma_knots, title, gpr)
    
    i = i + 1
    seq_ = seq_ + knots


# Noisy case ##-----------------------------------------------------------------

X = np.linspace(0.1, 9.9, 10)
X = np.atleast_2d(X).T

X = np.atleast_2d([5.4,2.8]).T

y = f(X).ravel()
dy = 0.5 + 1.0 * np.random.random(y.shape)
noise = np.random.normal(0, dy)
y += noise

kernel = Ck(1.0, (1e-3, 1e3)) * RBF(0.97, (1e-2, 1e2))
gpr = GaussianProcessRegressor(kernel=kernel, alpha=(dy / y) ** 2, optimizer=None)

# Fit to data using Maximum Likelihood Estimation of the parameters
gpr.fit(X, y)

# Make the prediction on the meshed x-axis (ask for MSE as well)
y_pred, sigma = gpr.predict(x, return_std=True)
sigma = np.round(sigma, 2)

title = 'GP Regression with 2 training points'
plot_gp_fit_predict(X, x, y, y_pred, sigma, title, gpr)
















