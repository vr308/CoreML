#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Sun Oct 15 16:41:51 2017
@author:  vr308
"""
import numpy as np
import matplotlib.pylab as plt
import scipy.stats as st
import GPy as gp
import george as grg
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C


M = 3
cov = np.identity(3)
x = np.sort(np.random.uniform(-8,8, 200))

# Sampling from a univariate gaussian

x=np.arange(-5,+5,0.01)
pdf=st.norm.pdf(x, loc=0,scale=1)
s = np.random.normal(0, 1, 25)

plt.figure()
plt.plot(x,pdf)
plt.title('Samples from a Gaussian distributed random variable $x \sim N(0,1)$', fontsize='x-small')
plt.xlabel('x')
plt.scatter(s,np.zeros(25), marker='x')

# Sampling from a GP prior 

k = gp.kern.RBF(input_dim=1,lengthscale=0.2)
k = gp.kern.Brownian()
k = gp.kern.Matern32(input_dim=1, variance=1, lengthscale=0.2)


# Select input points
X = np.linspace(0.,1.,500) # define X to be 500 points evenly spaced over [0,1]
X = X[:,None] 

# Set the mean and covariance function 

mu = np.zeros((500))
C = k.K(X,X)

# Generate 20 samples from the multivariate Gaussian with mean mu and covariance C

Z = np.random.multivariate_normal(mu,C,20)

plt.figure(figsize=(5,5))    
for i in range(20):
    plt.plot(X[:],Z[i,:])
plt.xlabel('x')
plt.ylabel('f(x)')
#plt.title('Samples from ' + r'$\mu(x) = 0$' + ' and '+ r'$k(x_{i}, x_{j}) = \frac{-1}{2l^{2}}(x_{i} - x_{j})^{2}$'  + ' and 'r'$l = 0.2$' + '[RBF kernel]', fontsize='x-small')
#plt.title('Samples from ' + r'$\mu(x) = 0$' + ' and '+ r'$k(x_{i}, x_{j}) = min(x_{i}, x_{j})$' + '[Brownian kernel]', fontsize='x-small')
plt.title('Samples from ' + r'$\mu(x) = 0$' + ' and '+ r'$k(x_{i}, x_{j}) = (1 + \sqrt{3}r)\exp(-\sqrt{3}r)$' + ' and 'r'$r = \sqrt{\sum_{i=1}^{n}\frac{x_{i} - y_{i}}{l_{i}^{2}}}$' + '[Matern32 kernel]', fontsize='x-small')

# Basis function representation of a GP

def sample_paths(x, cov, gamma):
    w=np.matrix(np.random.multivariate_normal([0,0,0],cov)).T
    Phi = np.matrix([phi_gauss(x, -4, gamma), phi_gauss(x, 0, gamma), phi_gauss(x, 4, gamma)]).T
    f_x = np.matmul(Phi,w)
    return f_x
    
def phi_gauss(x,m, gamma):
    return np.exp(-gamma*np.square(x - m))
    
   
mu = [-4,0,4.0]
gamma = [0.2,0.2,0.2]
x = np.sort(np.random.uniform(-8,8, 200))
plt.figure(figsize=(8,8))
plt.subplot(211)
for i in np.arange(0,len(mu)):
    plt.plot(x, phi_gauss(x,mu[i], gamma[i]), label= '($\mu_{%d}$,$\gamma_{%d}$) = [%d, %.2f]'%(i+1,i+1, mu[i], gamma[i]))
plt.title('Basis space $\Phi = [\phi_{1}(x), \phi_{2}(x), \phi_{3}(x)]$', fontsize='small')
plt.legend(fontsize='x-small')
plt.xlabel('$x$')
plt.ylabel('$\phi_{i}(x)$')
plt.subplot(212)
for i in np.arange(1,7):
    plt.plot(x, sample_paths(x, cov, gamma[0]))
plt.title('Sample curves $f(x) = \sum_{i=1}^{3}w_{i}\phi_{i}(x)$ generated by sampling $\mathbf{w} \sim N(0,1I)$ from its prior dist.', fontsize='small')
plt.xlabel('$x$')
plt.ylabel('$f(x)$')


# GP Regression (using GPY package - does not give error bars when noise is 0 aaaargh!)

X = np.linspace(0.05,0.95,10)[:,None]
Y = -np.cos(np.pi*X) + np.sin(4*np.pi*X)
Y_noise = -np.cos(np.pi*X) + np.sin(4*np.pi*X) + np.random.normal(loc=0.0, scale=0.1, size=(10,1)) 

k = gp.kern.RBF(input_dim=1, variance=1, lengthscale=1)
m = gp.models.GPRegression(X,Y,k)

m.Gaussian_noise = 0 #variance
m.rbf.lengthscale = 1
m.plot()
plt.title('lengthscale=0.2')
print(m)


# Sklearn version of GPs

# Noise-less case

def f(x):
    """The function to predict."""
    return x * np.sin(x)

# ----------------------------------------------------------------------
#  First the noiseless case
X = np.atleast_2d([1., 3., 5., 6., 7., 8.]).T

# Observations
y = f(X).ravel()

# Mesh the input space for evaluations of the real function, the prediction and
# its MSE
x = np.atleast_2d(np.linspace(0, 10, 1000)).T

# Instanciate a Gaussian Process model
kernel = RBF(length_scale=1.5)
gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=0)

# Fit to data using Maximum Likelihood Estimation of the parameters
gpr.fit(X, y)

# Make the prediction on the meshed x-axis (ask for MSE as well)
y_pred, sigma = gpr.predict(x, return_std=True)

# Plot the function, the prediction and the 95% confidence interval based on
# the MSE
fig = plt.figure()
plt.plot(x, f(x), 'r:', label=u'$f(x) = x\,\sin(x)$')
plt.plot(X, y, 'r.', markersize=10, label=u'Observations')
plt.plot(x, y_pred, 'b-', label=u'Prediction')
plt.fill(np.concatenate([x, x[::-1]]),
         np.concatenate([y_pred - 1.9600 * sigma,
                        (y_pred + 1.9600 * sigma)[::-1]]),
         alpha=.5, fc='b', ec='None', label='95% confidence interval')
plt.xlabel('$x$')
plt.ylabel('$f(x)$')
plt.ylim(-10, 20)
plt.legend(loc='upper left')



# Noisy case

X = np.linspace(0.1, 9.9, 10)
X = np.atleast_2d(X).T

#kernel = RBF(length_scale=1.2)
y = f(X).ravel()
dy = 0.5 + 1.0 * np.random.random(y.shape)
noise = np.random.normal(0, dy)
y += noise


kernel = C(1.0, (1e-3, 1e3)) * RBF(0.97, (1e-2, 1e2))
gpr = GaussianProcessRegressor(kernel=kernel, alpha=(dy / y) ** 2, optimizer=None)

# Fit to data using Maximum Likelihood Estimation of the parameters
gpr.fit(X, y)

# Make the prediction on the meshed x-axis (ask for MSE as well)
y_pred, sigma = gpr.predict(x, return_std=True)

# Plot the function, the prediction and the 95% confidence interval based on
# the MSE
fig = plt.figure()
plt.plot(x, f(x), 'r:', label=u'$f(x) = x\,\sin(x)$')
plt.scatter(X.ravel(), y, color='r', label=u'Observations')
plt.plot(x, y_pred, 'b-', label=u'Mean Prediction')
plt.fill(np.concatenate([x, x[::-1]]),
         np.concatenate([y_pred - 1.9600 * sigma,
                        (y_pred + 1.9600 * sigma)[::-1]]),
         alpha=.5, fc='b', ec='None', label='95% confidence interval')
#plt.fill(np.concatenate([x, x[::-1]]),
#         np.concatenate([y_pred - sigma.diagonal(),
#                        (y_pred + sigma.diagonal())[::-1]]),
#         alpha=.5, fc='b', ec='None', label='Variance')
plt.xlabel('$x$')
plt.ylabel('$f(x)$')
plt.ylim(-10, 20)
plt.legend(loc='upper left')


fig = plt.figure()
plt.plot(x, gpr.sample_y(x, 100), alpha=0.5)
plt.plot(x, y_pred, 'k--', label=u'Mean Prediction')
plt.scatter(X.ravel(), y, color='r', label=u'Observations', zorder=3)
plt.title('100 Samples from the posterior distribution')
plt.legend()





plt.title('lengthscale=0.2')